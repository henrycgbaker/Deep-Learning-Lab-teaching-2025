{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "  <h1>Architecture design in neural networks</h2>\n",
        "</div>\n",
        "\n",
        "Content:\n",
        "1. Writing a generic architectural constructor class (revision / extension of last week)\n",
        "2. Inspecting out training: TensorBoards intro (New!)\n",
        "3. Diagnosing Vanishing Gradients!\n",
        "4. Hyperparameter Tuning with Optuna lib"
      ],
      "metadata": {
        "id": "vzO4ORi0pnw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data\n",
        "\n",
        "Again, we will be using PyTorch library/framework for this lab, following on from last week"
      ],
      "metadata": {
        "id": "THR6SWeqMYR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import basic libs\n",
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# import torch (whole lib & specific modules)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "torch.set_default_dtype(torch.double)\n",
        "\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "vW8t4MQUpOfM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST( # MNIST for image classification\n",
        "    root=\"data\", # specifies directory\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(), # converts images from PIL format (or numpy array) to PyTorch Tensor (fundamental data structure for PyTorch)\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, #Â NB\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "ce8u3BGUxceR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there is unncessary amount of data!\n",
        "print(f\"len of training dataset: {len(training_data)}, len of testing dataset: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "cwYTKZJ_xmCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fb9b80-5665-4a6d-9563-242b6bfa4e08"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of training dataset: 60000, len of testing dataset: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract a subset of the datasets\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "training_data, _ = random_split(training_data, # arg: original dataset to split\n",
        "                                [1/10, 9/10], # split props\n",
        "                                generator=torch.manual_seed(seed)) # reproducibility\n",
        "\n",
        "test_data, _ = random_split(test_data, [1/10, 9/10], generator=torch.manual_seed(seed))\n",
        "\n",
        "print(f\"len of training dataset: {len(training_data)}, len of testing dataset: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "M4vLfzSCu7vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d8d8bc-5e1e-4754-ad45-fa13aeab8554"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of training dataset: 6000, len of testing dataset: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing the model using a generic class\n",
        "\n",
        "As last week, we construct a lazy sequential model with a customized number of `hidden_layers`, of dimensions provided by list object `hidden_size`.\n",
        "\n",
        "Whereas last week predefined a classification task -> only required `input_dims` as an arg; here we define a more generic architecture that can be either regr / classification -> requires `out_feat_size` that can take any hidden dims size."
      ],
      "metadata": {
        "id": "GT5-DpSRT1FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UiiTXAKh2QzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LazySequential(nn.Sequential):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_feat_size: int,\n",
        "                 out_feat_size: int,\n",
        "                 hidden_sizes: list,\n",
        "                 activation_fn: str ='ReLU'):\n",
        "\n",
        "        \"\"\"\n",
        "        NB: two step function:\n",
        "            1) first we build a list object called 'layers' that contains definition of layer-wise achitecture as iterable tuples\n",
        "            2) only then we do call constructor method of the parent class, feeding it the 'layers' list object\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        layers = [nn.Flatten()] # defines FIRST layer that flattens input image (channels * height * width) into vector\n",
        "\n",
        "        # loop adds INTERIM layers according to hidden_sizes list\n",
        "        for i, size in enumerate(hidden_sizes):\n",
        "\n",
        "            # 1st value in tuple:\n",
        "            hidden_layer = nn.Linear(in_features=(in_feat_size if i==0 # first layer\n",
        "                                                else hidden_sizes[i-1]), # subsequent layers\n",
        "                                   out_features=size)\n",
        "\n",
        "            # 2nd value in tuple:\n",
        "            layers.extend([hidden_layer, getattr(nn, activation_fn)()])\n",
        "                # ^^^ creates temp list w/ 2 elements\n",
        "                # ^^^ then adds tuple to 'layers' list\n",
        "\n",
        "        # finally the OUTPUT layer:\n",
        "        layers.append(nn.Linear(in_features=size, # NB: `size` = last value in hidden_sizes\n",
        "                                out_features=out_feat_size))\n",
        "\n",
        "        # now we unpack `layers` into as individual arguments into nn.Sequential constructor\n",
        "        super().__init__(*layers) # the * is the unpacking operator"
      ],
      "metadata": {
        "id": "13cY4sK5vwPu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e1af311a-af9e-4796-adc1-c8626d575c4b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    layers = [nn.Flatten()] # this flattens input image (channels * height * width) into vector\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have a generic class for definiing architecture upto the output activation function! We haven't applied a final activation function, because in PyTorch this is handled by the selected loss function (here, `CrossEntropyLoss`)\n",
        "\n",
        "Next, we need to define the training function; here our loss function will contain the final activations fn - this modularity allows us to define a generic architecture to be put to multiple end use cases.\n",
        "\n",
        "We will also feed our per-batch training loss (already calculated in the training loop) to a TensorBoard for visualisation."
      ],
      "metadata": {
        "id": "MRfGvJ0CjjsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the training function\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# NB: getattr gets attribute (fn/class/method) from object; syntax (object, attribute_str)\n",
        "\n",
        "def torch_train(model,\n",
        "                data,\n",
        "                optimizer='Adam',\n",
        "                loss='CrossEntropyLoss', # classification\n",
        "                batch_size=2**6,\n",
        "                epochs=1,\n",
        "                shuffle=False,\n",
        "                logdir=None): # for tensorboard\n",
        "\n",
        "\n",
        "  criterion = getattr(nn, loss)() # returns torch's CrossEntropyLoss() fn\n",
        "\n",
        "  optimizer = getattr(optim, optimizer)(model.parameters()) # returns 'Adam'() optimiser from torch.optim lib -> passes the parameter attr of the model\n",
        "\n",
        "  dataloader = DataLoader(data, # wraps our Dataset object in a dataloader\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # unique dir path for each training run\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  logdir = '//'.join(['runs', current_time, 'train']) if logdir is None else logdir\n",
        "\n",
        "  # SummaryWriter object (Torch class) for capturing data to be passed to TensorBoard\n",
        "  logger = SummaryWriter(logdir) # constructor takes str of dir to write events to\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # init values:\n",
        "    i = 0\n",
        "\n",
        "    for (X, Y) in tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        pred = model(X)\n",
        "        loss = criterion(\n",
        "            pred.squeeze(-1), # removes a dim -> passes to loss fn\n",
        "            Y.long()) # ensure Y is a long tensor for CrossEntropyLoss\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        i+=1 # as python starts 0\n",
        "\n",
        "        # here log training loss fed to TensorBoard\n",
        "        logger.add_scalar(tag='Training loss per batch', # --> name of scalar point added\n",
        "                          scalar_value=loss, # --> y (value of the loss)\n",
        "                          global_step=i + len(dataloader) * epoch) # --> x (batch number)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "5o76q5xmqtoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate our models"
      ],
      "metadata": {
        "id": "0J16wWfcvcub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define 2 architectures\n",
        "hidden_layers1 = [2**10, 2**5, 2**3]\n",
        "hidden_layers2 = [2**10, 2**7, 2**5]\n",
        "\n",
        "# model1\n",
        "model1 = LazySequential(in_feat_size=28*28,\n",
        "                        out_feat_size=10,\n",
        "                        hidden_sizes=hidden_layers1)\n",
        "model1 = torch_train(model1,\n",
        "                     training_data,\n",
        "                     logdir='runs/models1_2/model1')\n",
        "\n",
        "# model 2\n",
        "model2 = LazySequential(in_feat_size=28*28, out_feat_size=10, hidden_sizes=hidden_layers2)\n",
        "model2 = torch_train(model2, training_data, logdir='runs/models1_2/model2')"
      ],
      "metadata": {
        "id": "p9Y_xgNtne3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard(s)\n",
        "\n",
        "[Tensorboard](https://www.tensorflow.org/tensorboard) is a visualisation tool provided by TensorFlow that allows you to visualise our ML training process -> helps us understand how our model is training and to diagnose issues. Can incl metrics like:\n",
        "- loss\n",
        "- accuracy,\n",
        "- model graphs,\n",
        "- other data (all customisable).\n",
        "\n",
        "TensorBoard is just the visualisation dashboard; we need to generate and send the metrics to it. Tensorboard integrates w/ libs PyTorch (here: SummaryWriter) to calculate and log the specific metrics we want to see during your training process (i.e. we need to calculate the logged metrics in our training / eval loops)."
      ],
      "metadata": {
        "id": "gD2XQwPVYEhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab magic command -> loads IPython extension\n",
        "# tells tensorboard to look for log files in in `runs/models1_2` dir\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/models1_2"
      ],
      "metadata": {
        "id": "nlwVX9_33xQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This TensorBoard shows us the Training Loss that we calculated as a by-product of our training process; but it might be more informative to know how the average epoch loss changed over time.\n",
        "\n",
        "Let's develop our TensorBoard to visualise this. We'll need to calculate this new metric and pass it to the Torch's SummaryWriter object within the training function."
      ],
      "metadata": {
        "id": "oTGSsw_7bdFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Modifying training to log avg epoch loss and total elapsed time per epoch.\n",
        "\n",
        "import warnings\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def torch_train_mod(model, data, optimizer='Adam', loss='CrossEntropyLoss', batch_size=2**6, epochs=1, shuffle=False, logdir=None):\n",
        "\n",
        "    criterion = getattr(nn, loss)()\n",
        "    optimizer = getattr(optim, optimizer)(model.parameters())\n",
        "    dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    logdir = '//'.join(['runs', current_time, 'train']) if logdir is None else logdir\n",
        "    logger = SummaryWriter(logdir)\n",
        "\n",
        "\n",
        "    start = time.time() # <- NEW: returns current time as float\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0 # <- NEW: init epoch loss\n",
        "        i = 0\n",
        "        for (X, Y) in tqdm(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X)\n",
        "            loss = criterion(pred.squeeze(-1), torch.tensor(Y))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss+=loss.item() # <- NEW: sum of epoch loss\n",
        "            i+=1\n",
        "            logger.add_scalar('Training loss per batch', loss, i + len(dataloader) * epoch )\n",
        "\n",
        "        # new:\n",
        "        logger.add_scalar('Avg epoch loss',  # <- NEW comp avg\n",
        "                        epoch_loss / len(dataloader),\n",
        "                        epoch) # before was batch\n",
        "        logger.add_scalar('Total training time', time.time() - start, epoch)\n",
        "\n",
        "    logger.flush() # forces buffered data to disk\n",
        "    logger.close()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "m36Os3uevH9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Online learning, minibatch and batch\n",
        "\n",
        "With our training function set up to visualise how loss changes over epoch, let's visualise three common batch learning strategies.\n",
        "\n",
        "- **Online learning**: Processes one sample at a time, updating model weights after each individual example.\n",
        "- **Minibatch learning**: Processes minibatches (obviously).\n",
        "- **Batch learning** (or Full-batch learning): Processes the entire dataset at once, computing gradients over all samples before updating weights."
      ],
      "metadata": {
        "id": "Jsyj1KDJPF62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# define our learning types\n",
        "names = ['online', 'minibatch', 'fullbatch']\n",
        "batch_sizes = [1, 2**6, len(training_data)]\n",
        "\n",
        "# make one timestamped parent directory for this batch of runs\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "parent_logdir = f\"runs/batches/{timestamp}\"\n",
        "\n",
        "# first instatiate the model, then feed that model to the training function\n",
        "for name, batch_size in zip(names, batch_sizes): # zip combnes our two lists into an iterator of tuples\n",
        "    batch_model = LazySequential(\n",
        "        in_feat_size=28*28, # model is generic across batch types\n",
        "        out_feat_size=10,\n",
        "        hidden_sizes=hidden_layers1 # Using hidden_layers1 for consistency\n",
        "    )\n",
        "    logdir = f\"{parent_logdir}/{name}\"   # <- now grouped under timestamp\n",
        "    batch_model = torch_train_mod(\n",
        "        batch_model,\n",
        "        training_data,\n",
        "        batch_size=batch_size, # this changes within the loop\n",
        "        epochs=2,\n",
        "        logdir=logdir\n",
        "    )\n",
        "\n",
        "print(\"Logs saved to:\", parent_logdir)"
      ],
      "metadata": {
        "id": "fK1dMXEaS82h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir {parent_logdir}"
      ],
      "metadata": {
        "id": "qKfM_hUYeeyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Vanishing Gradient Problem:\n",
        "\n",
        "## A quick bit of theoretical context:\n",
        "\n",
        "In short, think of backpropagation as sending a **signal** backwards:\n",
        "* Each layer attenuates it by multiplying with <1.\n",
        "* By the time it reaches the start, itâs almost silence (vanishing gradient).\n",
        "\n",
        "---\n",
        "\n",
        "### NNs learn via **backpropagation**:\n",
        "\n",
        "  * Forward pass: compute activations $h^{(l)}$ layer by layer.\n",
        "  * Backward pass: compute gradients of the loss with respect to weights, using the chain rule.\n",
        "\n",
        "Formally, for layer $l$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial Loss}{\\partial W^{(l)}} = \\frac{\\partial Loss}{\\partial h^{(L)}} \\cdot\n",
        "\\frac{\\partial h^{(L)}}{\\partial h^{(L-1)}} \\cdots\n",
        "\\frac{\\partial h^{(l+1)}}{\\partial h^{(l)}} \\cdot\n",
        "\\frac{\\partial h^{(l)}}{\\partial W^{(l)}}\n",
        "$$\n",
        "\n",
        "This involves multiplying many Jacobians (derivs of activations wrt inputs).\n",
        "\n",
        "\n",
        "### Chain rule amplification / attenuation\n",
        "\n",
        "Each term in the product is typically a number between $(0,1)$ for common activation functions (sigmoid, tanh).\n",
        "\n",
        "* Example: derivative of sigmoid $\\sigma(z) = \\frac{1}{1+e^{-z}}$ is\n",
        "\n",
        "  $$\n",
        "  \\sigma'(z) = \\sigma(z)(1-\\sigma(z)) \\in (0, 0.25)\n",
        "  $$\n",
        "* For tanh, derivative $\\in (0,1)$, but max is 1 at the origin and usually smaller.\n",
        "\n",
        "So in backpropagation, the gradient passed backwards through each layer is repeatedly multiplied by numbers < 1.\n",
        "\n",
        "$$\n",
        "\\text{Gradient at layer } l \\approx \\prod_{k=l+1}^L f'(z^{(k)})\n",
        "$$\n",
        "\n",
        "If $L$ is large, this product shrinks **exponentially** with depth.\n",
        "\n",
        "\n",
        "### Vanishing gradient phenomenon\n",
        "\n",
        "In deep networks, gradients at the **earlier layers** (close to the input) become *vanishingly small*. This has consequences:\n",
        "  * Early layers barely update during training â they stay close to their initialisation.\n",
        "  * Network learns âshortcutsâ with later layers, but fails to capture hierarchical representations from raw input.\n",
        "\n",
        "## Solution & Historical context\n",
        "\n",
        "In the 1990s, this problem made it nearly impossible to train deep networks (> 2â3 layers). Then, breakthroughs:\n",
        "\n",
        "  * **ReLU activations** (avoid most saturation).\n",
        "  * **Batch normalisation** (keep activations in stable ranges).\n",
        "  * **Careful initialisation** (this built into most frameworks like PyTorch).\n",
        "  * **Residual connections (ResNets)**: skip connections let gradients flow more directly (see future classes).\n",
        "\n",
        "These techniques revived deep learning in the 2010s (woo!)\n"
      ],
      "metadata": {
        "id": "-CyTeBMjOwz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS:\n",
        "\n",
        "## Why itâs worse for sigmoid/tanh\n",
        "\n",
        "* Sigmoid squashes input to $[0,1]$. Most inputs fall in saturated regions ($\\sigma'(z) \\approx 0$).\n",
        "* Tanh squashes to $[-1,1]$. Better, but still saturates.\n",
        "* ReLU partially fixes it ($f'(z)=1$ when active, 0 when inactive).\n",
        "  * That prevents vanishing *for active neurons*, but âdead ReLUsâ still give zero gradient.\n",
        "\n",
        "---\n",
        "\n",
        "## Exploding gradient (the sibling problem)\n",
        "\n",
        "If derivatives or weight magnitudes > 1, the product **explodes exponentially**.\n",
        "\n",
        "* Early gradients become enormous â unstable updates.\n",
        "* Training loss oscillates or diverges.\n",
        "\n",
        "Vanishing and exploding are two sides of the same coin: *repeated multiplication through depth*.\n"
      ],
      "metadata": {
        "id": "ZBmm0JhMp_U3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Back to our lab implementation:\n",
        "\n",
        "The below exercise aims to demonstrate the vanishing gradient problem. By (i)initialising the weights to zero and (ii) using a Sigmoid activation function in a deep network, we observe how gradients diminish during training, hindering effective learning."
      ],
      "metadata": {
        "id": "UwgtxhMLqOZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights_zero(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        # init weights & biases to 0\n",
        "        torch.nn.init.zeros_(module.weight)\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "# instantiate custom layers\n",
        "zero_grad_layers = [2**5] * 50\n",
        "zero_grad_model = LazySequential(in_feat_size=28*28,\n",
        "                              out_feat_size=10,\n",
        "                              hidden_sizes=zero_grad_layers,\n",
        "                              activation_fn='Sigmoid')\n",
        "\n",
        "# recursively apply to all modules/layers in model\n",
        "_ = zero_grad_model.apply(init_weights_zero)\n",
        "\n",
        "# apply our train function (as per usual)\n",
        "zero_grad_model = torch_train(zero_grad_model, training_data, epochs=1, logdir='runs/zero_grad')"
      ],
      "metadata": {
        "id": "nzt4NRfsz5UP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2826eb3c-6294-450d-d7f0-b13ae17d7bbf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|ââââââââââ| 94/94 [00:02<00:00, 45.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/zero_grad"
      ],
      "metadata": {
        "id": "v2stNaFSeemJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see no trend in the loss time series above; it's purely stochastic / normally distributed. There's no meaningful learning going on when we encounter the vanishing gradient problem."
      ],
      "metadata": {
        "id": "KMJaq1FMLZi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to modify our training function to output a heatmap of the weights to the TensorBoard (same model as before, only diff training function)."
      ],
      "metadata": {
        "id": "01Scht8QJUV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_heatmaps(model, cmap='Reds', **fig_kwargs):\n",
        "    mat, titles = [], []\n",
        "    vmin, vmax = +np.inf, -np.inf\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      if len(param.squeeze().shape) == 2:\n",
        "        param = param.detach().numpy()\n",
        "        param = np.abs(param)\n",
        "        mat.append(param)\n",
        "        titles.append(name)\n",
        "\n",
        "        if param.max() > vmax:\n",
        "          vmax = param.max()\n",
        "        if param.min() < vmin:\n",
        "          vmin = param.min()\n",
        "\n",
        "    fig, axes = plt.subplots(2,3)\n",
        "    top, bottom = [1,2,3], [-4,-3,-2]\n",
        "    for i, weights in enumerate((top, bottom)):\n",
        "      for j, w in enumerate(weights):\n",
        "        axes[i,j].imshow(mat[w], vmin=vmin, vmax=vmax, cmap=cmap)\n",
        "\n",
        "\n",
        "    fig.suptitle('First 3 (top row) vs. last 3 (bottom row) hidden weights')\n",
        "    return fig"
      ],
      "metadata": {
        "id": "OohqlJL7qwa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc538567"
      },
      "source": [
        "This is going to show the weight matrix for selected layers.\n",
        "- Each cell in the heatmap corresponds to a single weight matrix, connecting  input features to output features in that layer.\n",
        "  - x axis: input features (units from prev layer)\n",
        "  - y axis: output features (units in current layer)\n",
        "- The colour of each cell indicates the magnitude of that weight.\n",
        "\n",
        "Next, we add it to out training function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train function to log heatmaps of the parameters at every epoch\n",
        "\n",
        "def torch_train_heatmap(model, data,\n",
        "                optimizer='Adam', loss='CrossEntropyLoss',\n",
        "                batch_size=2**6, epochs=1, shuffle=False, logdir=None,\n",
        "                cmap='Reds', **fig_kwargs):\n",
        "\n",
        "  criterion = getattr(nn, loss)()\n",
        "  optimizer = getattr(optim, optimizer)(model.parameters())\n",
        "  dataloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  logdir = '//'.join(['runs', current_time, 'train']) if logdir is None else logdir\n",
        "  logger = SummaryWriter(logdir)\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    avg_loss = 0\n",
        "    i = 0\n",
        "\n",
        "    for (X, Y) in tqdm(dataloader):\n",
        "\n",
        "        # NEW:\n",
        "        fig = weight_heatmaps(model, cmap=cmap, **fig_kwargs)\n",
        "        logger.add_figure('Weights', fig, global_step=i + len(dataloader) * epoch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred.squeeze(-1), torch.tensor(Y))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss+=loss.item()\n",
        "        i+=1\n",
        "        logger.add_scalar('Training loss', loss, i + len(dataloader) * epoch )\n",
        "\n",
        "    logger.add_scalar('Avg epoch loss', avg_loss / len(dataloader), epoch)\n",
        "    logger.add_scalar('Total training time', time.time() - start, epoch)\n",
        "\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "fKfT6pTWK5ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_grad_model = torch_train_heatmap(zero_grad_model, training_data, epochs=1, logdir='runs/heatmap')"
      ],
      "metadata": {
        "id": "yecnOVKaIGZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/heatmap"
      ],
      "metadata": {
        "id": "g7AR2JkEJkPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see weights/activations shrink layer by layer â gradient norms get smaller and smaller â earlier layers barely update â their weights stay close to initialisation (negligible coefficients)."
      ],
      "metadata": {
        "id": "L1pisLrTN7ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "Hyperparameter tuning is the process of systematically searching for the optimal set of hyperparameters (e.g. number of layers, size of the layers, learning rate, batch size, dropout ...) of a ML model. There are four common methods of hyperparameter optimization:  \n",
        "\n",
        "*   Manual\n",
        "*   Grid search\n",
        "*   Random search\n",
        "*   Bayesian search"
      ],
      "metadata": {
        "id": "uEenH2AkPYXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = [1,2,3]\n",
        "learning_rate = [0.1, 0.001, 0.00001]"
      ],
      "metadata": {
        "id": "JoYBELyJh_eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install optuna torcheval --q"
      ],
      "metadata": {
        "id": "PYdcfVRg5XXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna\n",
        "Optuna is an open-source hyperparameter optimization framework -> automates the process of finding the best set of hyperparameters. Optuna systematically searches through a defined space of possible values to find the combination that yields the best performance (e.g., highest accuracy, lowest loss) on our model.\n",
        "\n",
        "- **Study**: In Optuna, a \"study\" represents an optimisation session. Think of a study as a container that manages the entire hyperparameter tuning process.\n",
        "    - We specify the optimisation direction.\n",
        "        - 'maximize' for metrics like accuracy,\n",
        "        -  'minimize' for metrics like loss.\n",
        "\n",
        "- **Trial**: a single run of our model with a specific set of hyperparameters suggested by Optuna.\n",
        "    - The objective function defines what happens in each trial (w/ different parameters).\n",
        "    - Optuna calls this function repeatedly, each time providing a trial object.\n",
        "\n",
        "- **Objective Function**: This is the function that Optuna optimises.\n",
        "    - takes a trial object as input\n",
        "    - returns the metric we want to optimise (e.g., test accuracy).\n",
        "    - Inside the objective function:\n",
        "        - we use trial object to suggest values for the hyperparameters you want to tune. Optuna uses these suggestions to explore the hyperparameter space.\n",
        "        - we build and train our model using the suggested hyperparameters.\n",
        "        - we evaluate our model's performance using the chosen metric.\n",
        "        - we return the evaluated metric."
      ],
      "metadata": {
        "id": "a_ektsiFvMNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from torcheval.metrics.functional import multiclass_accuracy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define an objective function to be maximized.\n",
        "def objective(trial,\n",
        "              training_data,\n",
        "              test_data,\n",
        "              **train_params):\n",
        "    \"\"\"\n",
        "    Objective function for Optuna hyperparameter tuning.\n",
        "\n",
        "    Args:\n",
        "        trial (optuna.Trial): An Optuna trial object.\n",
        "        training_data (torch.utils.data.Dataset): The training dataset.\n",
        "        test_data (torch.utils.data.Dataset): The test dataset.\n",
        "        **train_params: Additional parameters to pass to the torch_train function.\n",
        "\n",
        "    Returns:\n",
        "        float: The test accuracy of the model with the suggested hyperparameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # Suggest values of the hyperparameters using a trial object.\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "        # ^^^ telling optuna to sample this hyperparam from defined bounds\n",
        "        # this is the magic: optuna will choose optimum sampling strategy to imporve performance based on past results\n",
        "    layers = []\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        size = trial.suggest_int(f'n_units_l{i}', 4, 128)\n",
        "        # ^^^ telling optuna to sample numb of hidden units from our bounds\n",
        "        layers.append(size)\n",
        "\n",
        "    # build models\n",
        "    model = LazySequential(in_feat_size=28*28, out_feat_size=10, hidden_sizes=layers).to(torch.device('cpu'))\n",
        "    model = torch_train(model, training_data, **train_params)\n",
        "\n",
        "    # prep data\n",
        "    test_dataloader = DataLoader(test_data, batch_size= len(test_data))\n",
        "\n",
        "    # compute acc\n",
        "    X_test, Y_test = next(iter(test_dataloader))\n",
        "    Y_pred = model(X_test).argmax(dim=-1)\n",
        "    test_acc = multiclass_accuracy(Y_pred, Y_test)\n",
        "\n",
        "    return test_acc"
      ],
      "metadata": {
        "id": "7YBdKLfGyLFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...with our objective function defined, let's create a study object"
      ],
      "metadata": {
        "id": "nJkVTmUyw6rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a study object\n",
        "study = optuna.create_study(direction='maximize')\n",
        "\n",
        "# Optimize the objective function.\n",
        "study.optimize(lambda trial: objective(trial, training_data, test_data), n_trials=5)\n",
        "# ^ lambda fn: takes arg trial, applies objective()"
      ],
      "metadata": {
        "id": "AViMrxeUxADz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe197ade"
      },
      "source": [
        "# Print the best trial's parameters and value\n",
        "print(\"Best trial:\")\n",
        "print(\"  Acc: {}\".format(study.best_trial.value))\n",
        "print(\"  Params: \")\n",
        "for key, value in study.best_trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "# can also access all trials\n",
        "print(\"\\nAll trials:\")\n",
        "for trial in study.trials:\n",
        "    print(\"  Trial {}:\".format(trial.number))\n",
        "    print(\"    Acc: {}\".format(trial.value))\n",
        "    print(\"    Params: {}\".format(trial.params))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}