{"cells":[{"cell_type":"markdown","id":"c48cb528","metadata":{"id":"c48cb528"},"source":["# Problem Set 3: Policy Text Classification w/ Open Source LLMs\n","\n","**Names**: [Your names here]  \n","**Team**: [Team name]  \n","**Dataset Choice**: [Option A or Option B]\n","\n","## Introduction\n","\n","In this assignment, you will:\n","1. Load and explore a climate policy text dataset\n","2. Test zero-shot classification with prompt engineering\n","3. Evaluate few-shot learning with examples\n","4. Fine-tune using LoRA (Low-Rank Adaptation)\n","5. Analyze errors and reflect on model performance\n","\n","**Important**: For the scope of this pset, it's acceptable if prompt engineering and few-shot learning don't drastically improve performance; your reflection on *why* matters more than achieving high scores.\n","\n","**Tip**: consider saving checkpoints of fine-tuned models (in task 4), as well as raw outputs into directories (for all tasks), to avoid having to rerun compute-expensive workflows repeatedly. This is generally good practice!"]},{"cell_type":"markdown","id":"587f80a0","metadata":{"id":"587f80a0"},"source":["## Setup and Installation"]},{"cell_type":"code","execution_count":null,"id":"f5028eed","metadata":{"id":"f5028eed"},"outputs":[],"source":["# Install required libraries\n","# !pip install datasets transformers torch peft accelerate evaluate scikit-learn"]},{"cell_type":"code","execution_count":null,"id":"78e86361","metadata":{"id":"78e86361"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    pipeline\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    hamming_loss,\n","    jaccard_score,\n","    classification_report,\n","    confusion_matrix\n",")\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seed for reproducibility\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","id":"b59a10d9","metadata":{"id":"b59a10d9"},"source":["## Configuration: Choose Your Dataset\n","\n","Uncomment ONE of the two options below based on your choice."]},{"cell_type":"code","execution_count":null,"id":"84ee073b","metadata":{"id":"84ee073b"},"outputs":[],"source":["# #TODO: Choose your dataset by uncommenting ONE option\n","\n","# # OPTION A: National Climate Targets\n","# DATASET_NAME = \"ClimatePolicyRadar/national-climate-targets\"\n","# MODEL_NAME = \"gpt2\"\n","# TARGET_MODULES = ['c_attn', 'c_proj']\n","# IS_MULTILABEL = True\n","# NUM_LABELS = 3\n","# LABEL_NAMES = ['Net Zero', 'Reduction', 'Other']\n","\n","# # OPTION B: TCFD Corporate Disclosure\n","# DATASET_NAME = \"climatebert/tcfd_recommendations\"\n","# MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","# TARGET_MODULES = ['q_proj', 'v_proj', 'k_proj', 'o_proj']\n","# IS_MULTILABEL = False\n","# NUM_LABELS = 5\n","# LABEL_NAMES = None  # Will be loaded from dataset\n","\n","print(f\"Dataset: {DATASET_NAME}\")\n","print(f\"Model: {MODEL_NAME}\")\n","print(f\"Task type: {'Multi-label' if IS_MULTILABEL else 'Single-label'}\")"]},{"cell_type":"markdown","id":"ce2dde04","metadata":{"id":"ce2dde04"},"source":["# Task 1: Data Loading and Exploration (10 points)\n","\n","**Goal**: Load your chosen dataset, understand its structure, and visualize label distributions.\n","\n","**TODO**:\n","1. Load the dataset from Hugging Face\n","2. Understand dataset structure and sizes\n","3. Analyse label distribution\n","4. Show 2-3 example texts with annotations\n","5. Create train/val/test splits\n","\n","NB: for option A you will need to apply a custom function that converts the annotation columns to a label list."]},{"cell_type":"code","execution_count":null,"id":"07b35b87","metadata":{"id":"07b35b87"},"outputs":[],"source":["# TODO here"]},{"cell_type":"markdown","id":"90a29aef","metadata":{"id":"90a29aef"},"source":["# Task 2: Zero-Shot Evaluation (15 points)\n","\n","**Goal**: Test your model without training using different prompt strategies.\n","\n","**TODO**:\n","1. Load model as text generator\n","2. Create 3+ prompt templates (direct, instructional, definition-based, etc.)\n","3. Implement parsing function to extract predictions from generated text\n","4. Evaluate each prompt on test set (sample 50-100 for speed)\n","5. Compare results and identify best prompt\n","6. **Written reflection**: Did prompt engineering help? Why or why not?"]},{"cell_type":"code","execution_count":null,"id":"8050b130","metadata":{"id":"8050b130"},"outputs":[],"source":["# TODO: Load tokenizer and create text generation pipeline\n","\n","tokenizer = None  # Load tokenizer\n","generator = None  # Create pipeline"]},{"cell_type":"code","execution_count":null,"id":"4ef13a55","metadata":{"id":"4ef13a55"},"outputs":[],"source":["# Create at least 4 different prompts\n","# for examples there's plenty of online documentation re: prompt design for API calls by OpenAI, HuggingFace, etc.:\n","\n","PROMPTS = {\n","    # these are just examples, feel free to modify as you see fit\n","    'direct': \"TODO: Your prompt here with {text} placeholder\",\n","    'instructional': \"TODO: Your prompt\",\n","    'definition': \"TODO: Your prompt\",\n","    'structured': \"TODO: Your prompt\",\n","    # others...\n","}"]},{"cell_type":"code","execution_count":null,"id":"c4b58065","metadata":{"id":"c4b58065"},"outputs":[],"source":["# #TODO: Implement parsing function\n","# Extract predicted label(s) from model's generated text\n","# depending on your prompt designs, you may need to adjust parsing logic\n","\n","def parse_output(generated_text):\n","    \"\"\"\n","    Parse model output to extract prediction.\n","\n","    For Option A: Return list [0/1, 0/1, 0/1] for [Net Zero, Reduction, Other]\n","    For Option B: Return integer 0-4 for class index\n","    \"\"\"\n","    # TODO: Implement parsing logic\n","    pass"]},{"cell_type":"code","execution_count":null,"id":"77812990","metadata":{"id":"77812990"},"outputs":[],"source":["# TODO: Evaluate zero-shot with each prompt\n","# Sample 100 test examples for speed\n","# For each prompt, get predictions and calculate metrics\n","\n","zero_shot_results = {}\n","\n","# TODO: Loop through prompts\n","# TODO: For each example, generate prediction\n","# TODO: Calculate accuracy and F1 score"]},{"cell_type":"code","execution_count":null,"id":"4e3e119e","metadata":{"id":"4e3e119e"},"outputs":[],"source":["# TODO: Compare prompt performance\n","# Create table or visualisation showing which prompt worked best"]},{"cell_type":"code","execution_count":null,"id":"6b8d9fe5","metadata":{"id":"6b8d9fe5"},"outputs":[],"source":["# #TODO: Print / plot best prompt type and its performance"]},{"cell_type":"markdown","id":"66069c81","metadata":{"id":"66069c81"},"source":["### Reflection: Zero-Shot Prompt Engineering\n","\n","**TODO**: Answer the following questions:\n","- Did prompt engineering improve performance compared to the direct prompt?\n","- If yes, which prompt design choices helped most?\n","- If no, why might prompting struggle on this task? Consider: model size, task complexity, text length, context windows\n","\n","[Write your reflection here]"]},{"cell_type":"markdown","id":"7ef20540","metadata":{"id":"7ef20540"},"source":["# Task 3: Few-Shot Evaluation (10 points)\n","\n","**Goal**: Test if providing examples in the prompt improves performance.\n","\n","**TODO**:\n","1. Select 2-5 training examples covering different labels\n","2. Create few-shot prompt with short examples (consider context window constraints for small models!)\n","3. Evaluate on test set\n","4. Compare with zero-shot\n","5. **Written reflection**: Did few-shot help? Why or why not?"]},{"cell_type":"code","execution_count":null,"id":"fe4fe7dc","metadata":{"id":"fe4fe7dc"},"outputs":[],"source":["# #TODO: Select few-shot examples\n","# Programmaticaly / manually select 3-5 examples from training set representing different labels\n","# IMPORTANT: consider length constraints of model context window\n","\n","few_shot_examples = []  # List of (text_snippet, label) tuples"]},{"cell_type":"code","execution_count":null,"id":"9cc6ce39","metadata":{"id":"9cc6ce39"},"outputs":[],"source":["\n","# #TODO: Create few-shot prompt template\n","# Include examples, then query text\n","# again, much documentation online for few-shot prompt design!\n","\n","def create_few_shot_prompt(test_text):\n","    \"\"\"Create prompt with few-shot examples.\"\"\"\n","    # TODO: Build prompt with examples + test text\n","    pass"]},{"cell_type":"code","execution_count":null,"id":"3813c90b","metadata":{"id":"3813c90b"},"outputs":[],"source":["# #TODO: Evaluate few-shot performance\n","# Use same 100 test examples as zero-shot\n","\n","few_shot_predictions = []\n","few_shot_true_labels = []\n","\n","# TODO: Generate predictions with few-shot prompt\n","\n","# TODO: Calculate metrics"]},{"cell_type":"code","execution_count":null,"id":"0edbfef1","metadata":{"id":"0edbfef1"},"outputs":[],"source":["# #TODO: Compare zero-shot vs few-shot\n","# Show side-by-side comparison of best zero-shot vs few-shot"]},{"cell_type":"markdown","id":"53ef8ecd","metadata":{"id":"53ef8ecd"},"source":["### Reflection: Few-Shot Learning\n","\n","**TODO**: Answer the following questions:\n","- Did few-shot learning improve over zero-shot?\n","- If no (or if it hurt performance), what might explain this? Consider: context window limits, example selection, model capabilities\n","- What challenges arise when using few-shot learning with long texts?\n","- When might few-shot be more effective?\n","\n","[Write your reflection here]"]},{"cell_type":"markdown","id":"1c273ea6","metadata":{"id":"1c273ea6"},"source":["# Task 4: LoRA Fine-Tuning (15 points)\n","\n","LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that keeps the original pretrained model frozen and injects small trainable rank-decomposed matrices into its weight layers. Instead of updating all model parameters, it learns low-rank updates that approximate the weight changes needed for a new task. This drastically reduces memory and compute requirements while maintaining performance close to full fine-tuning.\n","\n","**TODO**:\n","1. Prepare tokenized datasets\n","2. Load model for classification\n","3. Apply LoRA configuration\n","4. Train for 3-5+ epochs (/ as many as as deem necessary balancing loss reduction against time & compute constraints)\n","5. Plot learning curves\n","6. Evaluate on test set\n"]},{"cell_type":"code","execution_count":null,"id":"42b3ad34","metadata":{"id":"42b3ad34"},"outputs":[],"source":["# TODO: Prepare datasets for fine-tuning\n","# Tokenise texts and format labels\n","\n","def tokenize_function(examples):\n","    \"\"\"Tokenize texts and prepare labels.\"\"\"\n","    # TODO: Tokenize with padding and truncation\n","    # TODO: Add labels in correct format\n","    pass\n","\n","# TODO: Tokenize all splits"]},{"cell_type":"code","execution_count":null,"id":"60a95c06","metadata":{"id":"60a95c06"},"outputs":[],"source":["# TODO: Load model for sequence classification\n","\n","model = None  # Load AutoModelForSequenceClassification"]},{"cell_type":"code","execution_count":null,"id":"9dffa22f","metadata":{"id":"9dffa22f"},"outputs":[],"source":["# TODO: Apply LoRA configuration\n","\n","lora_config = LoraConfig(\n","    r=8,  # Rank\n","    lora_alpha=32,\n","    target_modules=TARGET_MODULES,\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_CLS\n",")\n","\n","# TODO: Apply LoRA to model\n","# TODO: Print trainable parameters"]},{"cell_type":"code","execution_count":null,"id":"90903142","metadata":{"id":"90903142"},"outputs":[],"source":["# TODO: Define training arguments\n","\n","training_args = TrainingArguments(\n","    # TODO: Add arguments\n",")"]},{"cell_type":"code","execution_count":null,"id":"414b7b4e","metadata":{"id":"414b7b4e"},"outputs":[],"source":["# TODO: Define compute_metrics function\n","\n","def compute_metrics(eval_pred):\n","    \"\"\"Calculate metrics for evaluation.\"\"\"\n","    logits, labels = eval_pred\n","\n","    # TODO: Get predictions from logits\n","    # For multi-label: Use sigmoid + threshold\n","    # For single-label: Use argmax\n","\n","    # TODO: Calculate accuracy, F1, etc.\n","\n","    return {\n","        'accuracy': 0.0,  # Replace with actual\n","        'f1': 0.0\n","    }"]},{"cell_type":"code","execution_count":null,"id":"cf6568d0","metadata":{"id":"cf6568d0"},"outputs":[],"source":["# TODO: Create Trainer and train\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tokenized,\n","    eval_dataset=val_tokenized,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# TODO: Train model\n","# train_result = trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"033e7791","metadata":{"id":"033e7791"},"outputs":[],"source":["# TODO: Plot learning curves\n","# Extract training history and plot loss/F1 over epochs"]},{"cell_type":"code","execution_count":null,"id":"3000bd58","metadata":{"id":"3000bd58"},"outputs":[],"source":["# TODO: Evaluate on test set\n","# accuracy & f1 necessary for both\n","# hamming loss and jaccard for multi-label"]},{"cell_type":"markdown","id":"8e5b8dc3","metadata":{"id":"8e5b8dc3"},"source":["# Task 5: Evaluation and Analysis (10 points)\n","\n","**Goal**: Analyse model performance, identify error patterns, and reflect on the full pipeline.\n","\n","**TODO**:\n","1. Create confusion matrix (Option B) or per-label breakdown (Option A)\n","2. Analyze 5-10 error cases from test set\n","3. Create error taxonomy table\n","4. Written reflection"]},{"cell_type":"code","execution_count":null,"id":"a9d1202b","metadata":{"id":"a9d1202b"},"outputs":[],"source":["# TODO: Get predictions on test set\n","\n","predictions = None  # Get final predictions\n","true_labels = None  # Get true labels"]},{"cell_type":"code","execution_count":null,"id":"0fa781d1","metadata":{"id":"0fa781d1"},"outputs":[],"source":["# TODO: Create confusion matrix or per-label metrics\n","\n","# Option B: Confusion matrix\n","# Option A: Per-label precision/recall/F1 table"]},{"cell_type":"code","execution_count":null,"id":"32747fd4","metadata":{"id":"32747fd4"},"outputs":[],"source":["# #TODO: Identify and analyse errors\n","# Find 5-10 misclassified examples\n","\n","# Find misclassified examples\n","errors = []\n","for i, (true, pred) in enumerate(zip(true_labels, predictions)):\n","    if not np.array_equal(true, pred):\n","        true_labels_list = [LABEL_NAMES[j] for j, val in enumerate(true) if val == 1]\n","        pred_labels_list = [LABEL_NAMES[j] for j, val in enumerate(pred) if val == 1]\n","\n","        errors.append({\n","            'index': i,\n","            'text': test_dataset[i]['text'],\n","            'true_labels': true_labels_list if true_labels_list else ['None'],\n","            'pred_labels': pred_labels_list if pred_labels_list else ['None'],\n","            'true_array': true,\n","            'pred_array': pred\n","        })\n","\n","print(f\"\\nTotal errors: {len(errors)} / {len(test_dataset)} ({len(errors)/len(test_dataset)*100:.1f}%)\")\n","\n","# TODO: For each, print text and consider what went wrong\n"]},{"cell_type":"code","execution_count":null,"id":"5c3fa93e","metadata":{"id":"5c3fa93e"},"outputs":[],"source":["# #TODO: Create error taxonomy\n","# Categorize errors by type (e.g., ambiguous language, rare class, etc.)\n","\n","# Categorize errors\n","error_categories = {\n","    # TODO complete custom categories\n","}\n","\n","for error in errors:\n","    true_set = set(error['true_labels'])\n","    pred_set = set(error['pred_labels'])\n","    # TODO categorisation logic\n","    if #...\n","    elif #...\n","    elif #...\n","\n","# Create taxonomy table\n","taxonomy_df = pd.DataFrame({\n","    'Error Category': list(error_categories.keys()),\n","    'Count': list(error_categories.values()),\n","    'Percentage': [v/len(errors)*100 for v in error_categories.values()]\n","})\n","\n","print(\"\\n\" + taxonomy_df.to_string(index=False))\n","\n","# TODO plot error distribution by category"]},{"cell_type":"markdown","id":"731e9530","metadata":{"id":"731e9530"},"source":["### Comprehensive Reflection\n","\n","**TODO**: Address the following questions in your reflection:\n","\n","1. What made this classification task difficult?\n","\n","2. Where did your model struggle most? Which classes/labels were hardest? Why?\n","\n","3. How did performance change from zero-shot → few-shot → fine-tuned?\n","   Was the progression what you expected?\n","\n","4. Why did (or didn't) prompt engineering and few-shot learning help?\n","\n","5. What common mistakes did your model make? Can you identify patterns? What could you try next to improve performance? Consider:\n","\n","6. What did you learn about using small LLMs for policy analysis?\n","   When are they sufficient vs. when do you need larger models or domain-specific training?\n","\n","[Write your reflection here (you may also do this programmatically if you wish)]"]}],"metadata":{"kernelspec":{"display_name":"deeplearning","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}