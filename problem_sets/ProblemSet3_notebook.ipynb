{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48cb528",
   "metadata": {
    "id": "c48cb528"
   },
   "source": [
    "# Problem Set 3: Policy Text Classification w/ Open Source LLMs\n",
    "\n",
    "**Names**: [Your names here]  \n",
    "**Team**: [Team name]  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, you will:\n",
    "1. Load and explore a climate policy text dataset\n",
    "2. Test zero-shot classification with prompt engineering\n",
    "3. Evaluate few-shot learning with examples\n",
    "4. Fine-tune using LoRA (Low-Rank Adaptation)\n",
    "5. Analyze errors and reflect on model performance\n",
    "\n",
    "**Important**: For the scope of this problem set, it is acceptable if prompt engineering and few-shot learning do not drastically improve performance; your reflection on *why* matters more than achieving high scores.\n",
    "\n",
    "**Tip**: consider saving checkpoints of fine-tuned models (in task 4), as well as raw outputs into directories (for all tasks), to avoid having to rerun compute-expensive workflows repeatedly. This is generally good practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f80a0",
   "metadata": {
    "id": "587f80a0"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5028eed",
   "metadata": {
    "id": "f5028eed"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "#!pip install datasets transformers torch peft accelerate evaluate scikit-learn numpy==1.26.4 matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e86361",
   "metadata": {
    "id": "78e86361"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# check for evaluable device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a10d9",
   "metadata": {
    "id": "b59a10d9"
   },
   "source": [
    "## Configuration: Loading Your Dataset \\& Model\n",
    "\n",
    "Run the following code to load your dataset & the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee073b",
   "metadata": {
    "id": "84ee073b"
   },
   "outputs": [],
   "source": [
    "# Climate Policy Radar's National Climate Targets\n",
    "DATASET_NAME = \"ClimatePolicyRadar/national-climate-targets\"\n",
    "MODEL_NAME = \"gpt2\"\n",
    "TARGET_MODULES = ['c_attn', 'c_proj']\n",
    "IS_MULTILABEL = True\n",
    "NUM_LABELS = 3\n",
    "LABEL_NAMES = ['Net Zero', 'Reduction', 'Other']\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Task type: {'Multi-label' if IS_MULTILABEL else 'Single-label'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dde04",
   "metadata": {
    "id": "ce2dde04"
   },
   "source": [
    "---\n",
    "\n",
    "# Task 1: Data Loading and Exploration (10 points)\n",
    "\n",
    "**Goal**: Load your chosen dataset, understand its structure, and visualise label distributions.\n",
    "\n",
    "**TODO**:\n",
    "1. Load the dataset from Hugging Face.\n",
    "2. Understand dataset structure and sizes.\n",
    "3. Analyse label distribution.\n",
    "4. Show sample texts from each possible combination of labels.\n",
    "5. Plot a histogram of the distribution of text lengths.\n",
    "6. Create train/val/test splits\n",
    "\n",
    "*Hint: you will need to apply a custom function that converts the annotation columns to a label list.*\n",
    "\n",
    "The point of this data wrangling is to understand how the dataset may pose challenges to our modelling. There's no need to write anything, but these exercises should hopefully help you when considering how best to leverage small open source LLMs for NLP policy analysis (and perhaps why they face challenges \\& limitations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b35b87",
   "metadata": {
    "id": "07b35b87"
   },
   "outputs": [],
   "source": [
    "# TODO here\n",
    "\n",
    "# be sure to define an object called `test_dataset`, as this will be used in later evaluation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427045e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a29aef",
   "metadata": {
    "id": "90a29aef"
   },
   "source": [
    "# Task 2: Zero-Shot Evaluation (15 points)\n",
    "\n",
    "**Goal**: Test GPT-2 using only some basic prompt engineering strategies.\n",
    "\n",
    "**TODO**:\n",
    "1. Tokenise the text \\& load model as text generator.\n",
    "2. Create 3+ programmatic prompt templates (direct, instructional, definition-based, key-word checklist, discriminatory vs generative, etc.).\n",
    "3. Implement parsing function to extract predictions from generated text.\n",
    "4. Evaluate each prompt on the test set (can sample 50-100 for speed).\n",
    "5. Compare results and identify best prompt.\n",
    "6. **Written reflection**: Did prompt engineering help? Why or why not? (150-300 words)\n",
    "\n",
    "*NB: to test this prompt engineering work, you will need to load using `\"text-generation\"` mode in the pipeline (i.e. not `\"text-classification\"`).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050b130",
   "metadata": {
    "id": "8050b130"
   },
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and create text generation pipeline\n",
    "\n",
    "tokenizer = None  # Load model's associated tokenizer: using HF's AutoTokenizer.from_pretrained\n",
    "generator = None  # Create pipeline: using HF's pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef13a55",
   "metadata": {
    "id": "4ef13a55"
   },
   "outputs": [],
   "source": [
    "# Create at least 4 different prompts\n",
    "# if stuck, there's plenty of online documentation re: prompt design for API calls by OpenAI, HuggingFace, etc.:\n",
    "\n",
    "PROMPTS = {\n",
    "    # these are just possible suggested types, feel free to modify as you see fit!\n",
    "    'direct': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    'instructional': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    'definition': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    'structured': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    # others...\n",
    "}\n",
    "\n",
    "print(f\"\\nCreated {len(PROMPTS)} ZERO-SHOT prompt templates\")\n",
    "print(\"\\nPrompt strategies tested:\")\n",
    "for i, name in enumerate(PROMPTS.keys(), 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8de95",
   "metadata": {},
   "source": [
    "Since it is a generative pipeline (not a classification pipeline), we need a parsing fn to assign class based on model's generative output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b58065",
   "metadata": {
    "id": "c4b58065"
   },
   "outputs": [],
   "source": [
    "# #TODO: Implement parsing function\n",
    "# Extract predicted label(s) from model's generated text\n",
    "# depending on your prompt designs, you may need to adjust parsing logic\n",
    "\n",
    "def parse_output(generated_text):\n",
    "    \"\"\"\n",
    "    Parse model output to extract prediction.\n",
    "    \"\"\"\n",
    "    # TODO: Implement parsing logic\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77812990",
   "metadata": {
    "id": "77812990"
   },
   "outputs": [],
   "source": [
    "# Evaluate zero-shot with each prompt\n",
    "\n",
    "# recommended to sample 50-100 examples from test set for speed\n",
    "EVAL_SAMPLES = #TODO\n",
    "\n",
    "# For each prompt, get predictions and calculate metrics (accuracy, F1 score, etc.)\n",
    "\n",
    "zero_shot_results = {} \n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66069c81",
   "metadata": {
    "id": "66069c81"
   },
   "source": [
    "### Reflection: Zero-Shot Prompt Engineering\n",
    "\n",
    "**TODO**: Answer the following:\n",
    "- Did prompt engineering improve performance compared to the direct prompt?\n",
    "- If yes, which prompt design choices helped most?\n",
    "- If no, why might prompting struggle on this task? Consider: model size, task complexity, text length, context windows\n",
    "\n",
    "[Write your reflection here] (~200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2debcb9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef20540",
   "metadata": {
    "id": "7ef20540"
   },
   "source": [
    "# Task 3: Few-Shot Evaluation (10 points)\n",
    "\n",
    "**Goal**: Test if providing examples in the prompt improves performance.\n",
    "\n",
    "**TODO**:\n",
    "1. Select 3-5 training examples covering different instructive label combinations\n",
    "2. Create few-shot prompt with short examples (consider context window constraints for small models!)\n",
    "3. Evaluate on test set\n",
    "4. Compare with zero-shot\n",
    "5. **Written reflection**: Did few-shot help? Why or why not? (150-300 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fe7dc",
   "metadata": {
    "id": "fe4fe7dc"
   },
   "outputs": [],
   "source": [
    "# Select few-shot examples\n",
    "\n",
    "few_shot_examples = []  # List of (text_snippet, label) tuples\n",
    "\n",
    "#TODO: either programmaticaly, or manually select examples from training set representing different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6ce39",
   "metadata": {
    "id": "9cc6ce39"
   },
   "outputs": [],
   "source": [
    "# Create few-shot prompt template.\n",
    "# Include examples, then query text; if you're stuck there's much documentation online for few-shot prompt design!\n",
    "\n",
    "def create_few_shot_prompt(test_text):\n",
    "    \"\"\"Create prompt with few-shot examples.\"\"\"\n",
    "    # TODO: Build prompt with examples + test text\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test different styles\n",
    "test_text = test_dataset[0]['text'] # or named as you defined it\n",
    "\n",
    "for style in []: # insert your types here\n",
    "    prompt = create_few_shot_prompt(test_text, style=style)\n",
    "    print(f\"\\n--- Style: {style} ---\")\n",
    "    print(f\"Length: {len(prompt)} chars\")\n",
    "    print(f\"Preview:\\n{prompt}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b1801",
   "metadata": {},
   "source": [
    "Tip: you may also need to define a new parser here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813c90b",
   "metadata": {
    "id": "3813c90b"
   },
   "outputs": [],
   "source": [
    "# Evaluate few-shot performance\n",
    "# Be sure to use the same 100 test examples as zero-shot\n",
    "\n",
    "few_shot_predictions = []\n",
    "few_shot_true_labels = []\n",
    "\n",
    "# TODO: Generate predictions with few-shot prompt\n",
    "\n",
    "# TODO: Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbfef1",
   "metadata": {
    "id": "0edbfef1"
   },
   "outputs": [],
   "source": [
    "# TODO: Compare zero-shot vs few-shot (either print or plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef8ecd",
   "metadata": {
    "id": "53ef8ecd"
   },
   "source": [
    "### Reflection: Few-Shot Learning\n",
    "\n",
    "**TODO**: Answer the following questions:\n",
    "- Did few-shot learning improve over zero-shot?\n",
    "- If no (or if it hurt performance), what might explain this? Consider: context window limits, example selection, model capabilities\n",
    "- What challenges arise when using few-shot learning with long texts?\n",
    "- When might few-shot be more effective?\n",
    "\n",
    "[Write your reflection here] (150-300 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e58be7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c273ea6",
   "metadata": {
    "id": "1c273ea6"
   },
   "source": [
    "# Task 4: LoRA Fine-Tuning (15 points)\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that keeps the original pretrained model frozen and injects small trainable rank-decomposed matrices into its weight layers. Instead of updating all model parameters, it learns low-rank updates that approximate the weight changes needed for a new task. This drastically reduces memory and compute requirements while maintaining performance close to full fine-tuning.\n",
    "\n",
    "Now we're getting a bit more hands on with out model, we can't just plug out tokenizer and GPT model into a HF pipeline.\n",
    "\n",
    "**TODO**:\n",
    "1. Prepare tokenised datasets \\& oad model for classification\n",
    "3. Apply LoRA configuration\n",
    "4. Train for 3-5+ epochs (/ as many as as deem necessary balancing loss reduction against time & compute constraints)\n",
    "5. Plot learning curves\n",
    "6. Evaluate on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b3ad34",
   "metadata": {
    "id": "42b3ad34"
   },
   "outputs": [],
   "source": [
    "# TODO: Prepare datasets for fine-tuning\n",
    "# Tokenize texts and format labels using the tokenizer defined above\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize texts and prepare labels.\"\"\"\n",
    "    # TODO: Tokenize with padding and truncation\n",
    "    # TODO: Add labels in correct format\n",
    "    pass\n",
    "\n",
    "# TODO: Tokenize all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a95c06",
   "metadata": {
    "id": "60a95c06"
   },
   "outputs": [],
   "source": [
    "# TODO: Load model for sequence classification\n",
    "\n",
    "model = None  # Load using AutoModelForSequenceClassification\n",
    "# hint you'll need problem_type=\"multi_label_classification\" & the number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffa22f",
   "metadata": {
    "id": "9dffa22f"
   },
   "outputs": [],
   "source": [
    "# Apply LoRA configuration (just run this code as is, no edits required)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90903142",
   "metadata": {
    "id": "90903142"
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # TODO: Add arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b7b4e",
   "metadata": {
    "id": "414b7b4e"
   },
   "outputs": [],
   "source": [
    "# TODO: Define compute_metrics function\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate metrics for evaluation.\"\"\"\n",
    "    \n",
    "    # Hint: for multi-label: Use sigmoid + threshold\n",
    "    # Hint: for mult-label problems, consider using the following sklearn metrics:\n",
    "    # - accuracy_score()\n",
    "    # - f1_score() with average='macro' or 'weighted'\n",
    "    # - hamming_loss()\n",
    "    # - jaccard_score() with average='samples'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6568d0",
   "metadata": {
    "id": "cf6568d0"
   },
   "outputs": [],
   "source": [
    "# Create Trainer and train\n",
    "\n",
    "trainer = Trainer(\n",
    "    #TODO: add required arguments\n",
    ")\n",
    "\n",
    "#TODO: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e7791",
   "metadata": {
    "id": "033e7791"
   },
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "# TODO: Extract training history and plot loss/F1 over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000bd58",
   "metadata": {
    "id": "3000bd58"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81253f76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b8dc3",
   "metadata": {
    "id": "8e5b8dc3"
   },
   "source": [
    "# Task 5: Evaluation and Analysis (10 points)\n",
    "\n",
    "**Goal**: Analyse model performance, identify error patterns, and reflect on the full pipeline.\n",
    "\n",
    "### But First, Setting a Baseline: Benchmarking Against Climate Policy Radar's Downstream Model\n",
    "\n",
    "Before fully evaluating our GPT-2 efforts, let's first establish a baseline using Climate Policy Radar's existing Hugging Face model. This is a domain-specific model pre-fine tuned to this dataset, and will help us benchmark our GPT-2 experiments.\n",
    "*NB: another option could have been to use ClimateBERT's base model, plus a bit of fine tuning, but since this dataset has a dedicated fine-tuned model, let's use that.*\n",
    "\n",
    "**Key difference**: This model uses a **text-classification pipeline** (direct label prediction), whereas GPT-2 uses a **text-generation pipeline** (generating text that we must parse). Bear in mind that the workflow required to complete your tasks above, will thus look different from what we've done here. Be sure to run this AFTER completing Task 1 as you need `test_dataset` defined.\n",
    "\n",
    "We will provide the code to generate benchmark predictions, you will then include these benchmark predictions into your evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ClimateBERT classifier\n",
    "benchmark_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"ClimatePolicyRadar/national-climate-targets\",\n",
    "    device=DEVICE,\n",
    "    top_k=None  # returnS all label scores\n",
    ")\n",
    "\n",
    "print(\"\\Benchmark loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sample for baseline evaluation\n",
    "test_sample_baseline = test_dataset.shuffle(seed=SEED).select(range(min(EVAL_SAMPLES, len(test_dataset))))\n",
    "\n",
    "brenchmark_predictions = []\n",
    "brenchmark_true_labels = []\n",
    "\n",
    "for i, example in enumerate(test_sample_baseline):\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"  Processing {i+1}/{len(test_sample_baseline)}...\", end='\\r')\n",
    "\n",
    "    # can handle longer texts, but we truncate for consistency\n",
    "    text = example['text'][:512]\n",
    "\n",
    "    try:\n",
    "        # get preds\n",
    "        # output format: list of dicts with 'label' and 'score' keys\n",
    "        output = benchmark_classifier(text)\n",
    "\n",
    "        # convert output to binary multi-label format [Net Zero, Reduction, Other]\n",
    "        pred = [0, 0, 0]\n",
    "        for label_score in output[0]:\n",
    "            label_name = label_score['label']\n",
    "            score = label_score['score']\n",
    "\n",
    "            # apply threshold and map to our label indices\n",
    "            if 'net' in label_name.lower() and 'zero' in label_name.lower() and score > 0.5:\n",
    "                pred[0] = 1\n",
    "            elif 'reduction' in label_name.lower() and score > 0.5:\n",
    "                pred[1] = 1\n",
    "            elif 'other' in label_name.lower() and score > 0.5:\n",
    "                pred[2] = 1\n",
    "\n",
    "        brenchmark_predictions.append(pred)\n",
    "        brenchmark_true_labels.append(example['labels'])\n",
    "\n",
    "    except Exception as e:\n",
    "        # if prediction fails, predict no labels\n",
    "        brenchmark_predictions.append([0, 0, 0])\n",
    "        brenchmark_true_labels.append(example['labels'])\n",
    "\n",
    "print(f\"  Processing {len(test_sample_baseline)}/{len(test_sample_baseline)}... Done!\")\n",
    "\n",
    "# numpy arrays for evaluation\n",
    "brenchmark_predictions = np.array(brenchmark_predictions)\n",
    "brenchmark_true_labels = np.array(brenchmark_true_labels)\n",
    "\n",
    "print(f\"\\n✓ Baseline predictions complete\")\n",
    "print(f\"  Predictions shape: {brenchmark_predictions.shape}\")\n",
    "print(f\"  True labels shape: {brenchmark_true_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb237ae",
   "metadata": {},
   "source": [
    "The above code provides you with `benchmark_predictions` and `benchmark_true_labels` as objects to incorporate into your evaluation.\n",
    "\n",
    "**TODO**:\n",
    "1. Examine/plot model performance (acc, f1-score, jaccard-score) of the zero-shot, few-shot vs LoRA fine tuned, against the benchmark model performance.\n",
    "2. For the best-performing of your models/prompts, programatically analyse error cases from the test set. Most of this is pre-implemented for you below:\n",
    "    - identify misclassified examples (mostly pre-implemented)\n",
    "    - programmatically define if they are complete miss, partial errors, false positives, or false negatives (mostly pre-implemented)\n",
    "    - display/print/plot a count \\& percentage of total for each error type\n",
    "4. Written a reflection (150-300 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1202b",
   "metadata": {
    "id": "a9d1202b"
   },
   "outputs": [],
   "source": [
    "# TODO: compare between zero-shot, few-shot, and fine-tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32747fd4",
   "metadata": {
    "id": "32747fd4"
   },
   "outputs": [],
   "source": [
    "# #TODO: Identify and analyse errors for best performing model\n",
    "\n",
    "# Find misclassified examples\n",
    "errors = []\n",
    "for i, (true, pred) in enumerate(zip(true_labels, predictions)):\n",
    "    if not np.array_equal(true, pred):\n",
    "        true_labels_list = [LABEL_NAMES[j] for j, val in enumerate(true) if val == 1]\n",
    "        pred_labels_list = [LABEL_NAMES[j] for j, val in enumerate(pred) if val == 1]\n",
    "\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'text': test_dataset[i]['text'],\n",
    "            'true_labels': true_labels_list if true_labels_list else ['None'], # human readable\n",
    "            'pred_labels': pred_labels_list if pred_labels_list else ['None'],\n",
    "            'true_array': true, # machine readable\n",
    "            'pred_array': pred\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)} / {len(test_dataset)} ({len(errors)/len(test_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fa93e",
   "metadata": {
    "id": "5c3fa93e"
   },
   "outputs": [],
   "source": [
    "# #TODO: Create error taxonomy for best performing model\n",
    "\n",
    "# Categorize errors\n",
    "error_categories = {\n",
    "    'False Negative (missed label)': 0,\n",
    "    'False Positive (extra label)': 0,\n",
    "    'Complete miss': 0,\n",
    "    'Partial (mixed FP/FN)': 0\n",
    "}\n",
    "\n",
    "for error in errors:\n",
    "    true_set = set(error['true_labels'])\n",
    "    pred_set = set(error['pred_labels'])\n",
    "    # TODO categorisation logic\n",
    "    if #...\n",
    "    elif #...\n",
    "    elif #...\n",
    "\n",
    "# Create taxonomy table\n",
    "taxonomy_df = pd.DataFrame({\n",
    "    'Error Category': list(error_categories.keys()),\n",
    "    'Count': list(error_categories.values()),\n",
    "    'Percentage': [v/len(errors)*100 for v in error_categories.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + taxonomy_df.to_string(index=False))\n",
    "\n",
    "# TODO plot error distribution by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include any further plots / analysis as you see fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e9530",
   "metadata": {
    "id": "731e9530"
   },
   "source": [
    "### Overall Reflection\n",
    "\n",
    "**TODO**: possible questions you could consider:\n",
    "\n",
    "1. Where did your model struggle most? Which classes/labels were hardest? Why?\n",
    "2. How did performance change from zero-shot → few-shot → fine-tuned? Was the progression what you expected?\n",
    "3. How did your attempts compare to the benchmark HF model?\n",
    "3. Why did (or didn't) prompt engineering and few-shot learning help?\n",
    "4. What common mistakes did your model make? Can you identify patterns? What could you try next to improve performance? Consider:\n",
    "4. What did you learn about using small LLMs for policy analysis? When are they sufficient vs. when do you need larger models or domain-specific training?  \n",
    "\n",
    "[Write your reflection here] (150-300 words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
