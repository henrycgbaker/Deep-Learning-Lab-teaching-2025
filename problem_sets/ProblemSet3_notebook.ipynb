{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48cb528",
   "metadata": {
    "id": "c48cb528"
   },
   "source": [
    "# Problem Set 3: Policy Text Classification w/ Open Source LLMs\n",
    "\n",
    "**Names**: [Your names here]  \n",
    "**Team**: [Team name]  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, you will:\n",
    "1. Load and explore a climate policy text dataset\n",
    "2. Test zero-shot classification with prompt engineering\n",
    "3. Evaluate few-shot learning with examples\n",
    "4. Fine-tune using LoRA (Low-Rank Adaptation)\n",
    "5. Analyze errors and reflect on model performance\n",
    "\n",
    "**Important**: For the scope of this problem set, it is acceptable if prompt engineering and few-shot learning do not drastically improve performance; your reflection on *why* matters more than achieving high scores.\n",
    "\n",
    "**Tip**: consider saving checkpoints of fine-tuned models (in task 4), as well as raw outputs into directories (for all tasks), to avoid having to rerun compute-expensive workflows repeatedly. This is generally good practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f80a0",
   "metadata": {
    "id": "587f80a0"
   },
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5028eed",
   "metadata": {
    "id": "f5028eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: transformers in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: peft in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: accelerate in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: evaluate in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (3.10.7)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: sympy in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/henrybaker/miniconda3/envs/deep_learning/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "#!pip install datasets transformers torch peft accelerate evaluate scikit-learn numpy==1.26.4 matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e86361",
   "metadata": {
    "id": "78e86361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# check for evaluable device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a10d9",
   "metadata": {
    "id": "b59a10d9"
   },
   "source": [
    "## Configuration: Loading Your Dataset \\& Model\n",
    "\n",
    "Run the following code to load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ee073b",
   "metadata": {
    "id": "84ee073b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ClimatePolicyRadar/national-climate-targets\n",
      "Model: gpt2\n",
      "Task type: Multi-label\n"
     ]
    }
   ],
   "source": [
    "# Climate Policy Radar's National Climate Targets\n",
    "DATASET_NAME = \"ClimatePolicyRadar/national-climate-targets\"\n",
    "MODEL_NAME = \"gpt2\"\n",
    "TARGET_MODULES = ['c_attn', 'c_proj']\n",
    "IS_MULTILABEL = True\n",
    "NUM_LABELS = 3\n",
    "LABEL_NAMES = ['Net Zero', 'Reduction', 'Other']\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Task type: {'Multi-label' if IS_MULTILABEL else 'Single-label'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2dde04",
   "metadata": {
    "id": "ce2dde04"
   },
   "source": [
    "---\n",
    "\n",
    "# Task 1: Data Loading and Exploration (10 points)\n",
    "\n",
    "**Goal**: Load your chosen dataset, understand its structure, and visualise label distributions.\n",
    "\n",
    "**TODO**:\n",
    "1. Load the dataset from Hugging Face.\n",
    "2. Understand dataset structure and sizes.\n",
    "3. Analyse label distribution.\n",
    "4. Show sample texts from each possible combination of labels.\n",
    "5. Plot a histogram of the distribution of text lengths.\n",
    "6. Create train/val/test splits\n",
    "\n",
    "*Hint: you will need to apply a custom function that converts the annotation columns to a label list.*\n",
    "\n",
    "The point of this data wrangling is to understand how the dataset may pose challenges to our modelling. There's no need to write anything, but these exercises should hopefully help you when considering how best to leverage small open source LLMs for NLP policy analysis (and perhaps why they face challenges \\& limitations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b35b87",
   "metadata": {
    "id": "07b35b87"
   },
   "outputs": [],
   "source": [
    "# TODO here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a29aef",
   "metadata": {
    "id": "90a29aef"
   },
   "source": [
    "---\n",
    "\n",
    "# Task 2: Zero-Shot Evaluation (15 points)\n",
    "\n",
    "**Goal**: Test GPT-2 using only some basic prompt engineering strategies.\n",
    "\n",
    "**TODO**:\n",
    "1. Tokenise the text \\& load model as text generator.\n",
    "2. Create 3+ programmatic prompt templates (direct, instructional, definition-based, key-word checklist, discriminatory vs generative, etc.).\n",
    "3. Implement parsing function to extract predictions from generated text.\n",
    "4. Evaluate each prompt on the test set (can sample 50-100 for speed).\n",
    "5. Compare results and identify best prompt.\n",
    "6. **Written reflection**: Did prompt engineering help? Why or why not? (150-300 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050b130",
   "metadata": {
    "id": "8050b130"
   },
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and create text generation pipeline\n",
    "\n",
    "tokenizer = None  # Load tokenizer: using HF's AutoTokenizer.from_pretrained\n",
    "generator = None  # Create pipeline: using HF's pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef13a55",
   "metadata": {
    "id": "4ef13a55"
   },
   "outputs": [],
   "source": [
    "# Create at least 4 different prompts\n",
    "# if stuck, there's plenty of online documentation re: prompt design for API calls by OpenAI, HuggingFace, etc.:\n",
    "\n",
    "PROMPTS = {\n",
    "    # these are just examples, feel free to modify as you see fit\n",
    "    'direct': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    'instructional': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    'definition': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    'structured': \"TODO: Your prompt here with {text} placeholder\",\n",
    "    # others...\n",
    "}\n",
    "\n",
    "print(f\"\\nCreated {len(PROMPTS)} ZERO-SHOT prompt templates\")\n",
    "print(\"\\nPrompt strategies tested:\")\n",
    "for i, name in enumerate(PROMPTS.keys(), 1):\n",
    "    print(f\"  {i:2d}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b58065",
   "metadata": {
    "id": "c4b58065"
   },
   "outputs": [],
   "source": [
    "# #TODO: Implement parsing function\n",
    "# Extract predicted label(s) from model's generated text\n",
    "# depending on your prompt designs, you may need to adjust parsing logic\n",
    "\n",
    "def parse_output(generated_text):\n",
    "    \"\"\"\n",
    "    Parse model output to extract prediction.\n",
    "    \"\"\"\n",
    "    # TODO: Implement parsing logic\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77812990",
   "metadata": {
    "id": "77812990"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate zero-shot with each prompt\n",
    "# recommended to sample 50-100 examples from test set for speed\n",
    "# For each prompt, get predictions and calculate metrics (accuracy, F1 score, etc.)\n",
    "\n",
    "zero_shot_results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66069c81",
   "metadata": {
    "id": "66069c81"
   },
   "source": [
    "### Reflection: Zero-Shot Prompt Engineering\n",
    "\n",
    "**TODO**: Answer the following questions:\n",
    "- Did prompt engineering improve performance compared to the direct prompt?\n",
    "- If yes, which prompt design choices helped most?\n",
    "- If no, why might prompting struggle on this task? Consider: model size, task complexity, text length, context windows\n",
    "\n",
    "[Write your reflection here] (~200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef20540",
   "metadata": {
    "id": "7ef20540"
   },
   "source": [
    "# Task 3: Few-Shot Evaluation (10 points)\n",
    "\n",
    "**Goal**: Test if providing examples in the prompt improves performance.\n",
    "\n",
    "**TODO**:\n",
    "1. Select 2-5 training examples covering different labels\n",
    "2. Create few-shot prompt with short examples (consider context window constraints for small models!)\n",
    "3. Evaluate on test set\n",
    "4. Compare with zero-shot\n",
    "5. **Written reflection**: Did few-shot help? Why or why not? (150-300 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fe7dc",
   "metadata": {
    "id": "fe4fe7dc"
   },
   "outputs": [],
   "source": [
    "# #TODO: Select few-shot examples\n",
    "# Programmaticaly / manually select 3-5 examples from training set representing different labels\n",
    "\n",
    "few_shot_examples = []  # List of (text_snippet, label) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc6ce39",
   "metadata": {
    "id": "9cc6ce39"
   },
   "outputs": [],
   "source": [
    "# #TODO: Create few-shot prompt template. Include examples, then query text; if you're stuck there's much documentation online for few-shot prompt design!\n",
    "\n",
    "def create_few_shot_prompt(test_text):\n",
    "    \"\"\"Create prompt with few-shot examples.\"\"\"\n",
    "    # TODO: Build prompt with examples + test text\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3813c90b",
   "metadata": {
    "id": "3813c90b"
   },
   "outputs": [],
   "source": [
    "# #TODO: Evaluate few-shot performance\n",
    "# Be sure to use the same 100 test examples as zero-shot\n",
    "\n",
    "few_shot_predictions = []\n",
    "few_shot_true_labels = []\n",
    "\n",
    "# TODO: Generate predictions with few-shot prompt\n",
    "\n",
    "# TODO: Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbfef1",
   "metadata": {
    "id": "0edbfef1"
   },
   "outputs": [],
   "source": [
    "# #TODO: Compare zero-shot vs few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef8ecd",
   "metadata": {
    "id": "53ef8ecd"
   },
   "source": [
    "### Reflection: Few-Shot Learning\n",
    "\n",
    "**TODO**: Answer the following questions:\n",
    "- Did few-shot learning improve over zero-shot?\n",
    "- If no (or if it hurt performance), what might explain this? Consider: context window limits, example selection, model capabilities\n",
    "- What challenges arise when using few-shot learning with long texts?\n",
    "- When might few-shot be more effective?\n",
    "\n",
    "[Write your reflection here] (150-300 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c273ea6",
   "metadata": {
    "id": "1c273ea6"
   },
   "source": [
    "# Task 4: LoRA Fine-Tuning (15 points)\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that keeps the original pretrained model frozen and injects small trainable rank-decomposed matrices into its weight layers. Instead of updating all model parameters, it learns low-rank updates that approximate the weight changes needed for a new task. This drastically reduces memory and compute requirements while maintaining performance close to full fine-tuning.\n",
    "\n",
    "Now we're getting a bit more hands on with out model, we can't just plug out tokenizer and GPT model into a HF pipeline.\n",
    "\n",
    "**TODO**:\n",
    "1. Prepare tokenised datasets \\& oad model for classification\n",
    "3. Apply LoRA configuration\n",
    "4. Train for 3-5+ epochs (/ as many as as deem necessary balancing loss reduction against time & compute constraints)\n",
    "5. Plot learning curves\n",
    "6. Evaluate on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3ad34",
   "metadata": {
    "id": "42b3ad34"
   },
   "outputs": [],
   "source": [
    "# TODO: Prepare datasets for fine-tuning\n",
    "# Tokenize texts and format labels using the tokenizer from above\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize texts and prepare labels.\"\"\"\n",
    "    # TODO: Tokenize with padding and truncation\n",
    "    # TODO: Add labels in correct format\n",
    "    pass\n",
    "\n",
    "# TODO: Tokenize all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a95c06",
   "metadata": {
    "id": "60a95c06"
   },
   "outputs": [],
   "source": [
    "# TODO: Load model for sequence classification\n",
    "\n",
    "model = None  # Load AutoModelForSequenceClassification\n",
    "# hint you'll need problem_type=\"multi_label_classification\" & the number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffa22f",
   "metadata": {
    "id": "9dffa22f"
   },
   "outputs": [],
   "source": [
    "# TODO: Apply LoRA configuration\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90903142",
   "metadata": {
    "id": "90903142"
   },
   "outputs": [],
   "source": [
    "# TODO: Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # TODO: Add arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b7b4e",
   "metadata": {
    "id": "414b7b4e"
   },
   "outputs": [],
   "source": [
    "# TODO: Define compute_metrics function\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate metrics for evaluation.\"\"\"\n",
    "    \n",
    "    # Hint: for multi-label: Use sigmoid + threshold\n",
    "    # Hint: for mult-label problems, consider using the following sklearn metrics:\n",
    "    # - accuracy_score()\n",
    "    # - f1_score() with average='macro' or 'weighted'\n",
    "    # - hamming_loss()\n",
    "    # - jaccard_score() with average='samples'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6568d0",
   "metadata": {
    "id": "cf6568d0"
   },
   "outputs": [],
   "source": [
    "# TODO: Create Trainer and train\n",
    "\n",
    "trainer = Trainer(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e7791",
   "metadata": {
    "id": "033e7791"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot learning curves\n",
    "# Extract training history and plot loss/F1 over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000bd58",
   "metadata": {
    "id": "3000bd58"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b8dc3",
   "metadata": {
    "id": "8e5b8dc3"
   },
   "source": [
    "# Task 5: Evaluation and Analysis (10 points)\n",
    "\n",
    "**Goal**: Analyse model performance, identify error patterns, and reflect on the full pipeline.\n",
    "\n",
    "**TODO**:\n",
    "1. Examine/plot model performance (acc, f1-score, jaccard-score) of the zero-shot, few-shot vs LoRA fine tuned,\n",
    "2. For the best-performing model, programatically analyse error cases from the test set. Most of this is pre-implemented for you below:\n",
    "    - identify misclassified examples (mostly pre-implemented)\n",
    "    - programmatically define if they are complete miss, partial errors, false positives, or false negatives (mostly pre-implemented)\n",
    "    - display/print/plot a count \\& percentage of total for each error type\n",
    "4. Written a reflection on the full pipeline (150-300 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1202b",
   "metadata": {
    "id": "a9d1202b"
   },
   "outputs": [],
   "source": [
    "# TODO: compare between zero-shot, few-shot, and fine-tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32747fd4",
   "metadata": {
    "id": "32747fd4"
   },
   "outputs": [],
   "source": [
    "# #TODO: Identify and analyse errors for best performing model\n",
    "\n",
    "# Find misclassified examples\n",
    "errors = []\n",
    "for i, (true, pred) in enumerate(zip(true_labels, predictions)):\n",
    "    if not np.array_equal(true, pred):\n",
    "        true_labels_list = [LABEL_NAMES[j] for j, val in enumerate(true) if val == 1]\n",
    "        pred_labels_list = [LABEL_NAMES[j] for j, val in enumerate(pred) if val == 1]\n",
    "\n",
    "        errors.append({\n",
    "            'index': i,\n",
    "            'text': test_dataset[i]['text'],\n",
    "            'true_labels': true_labels_list if true_labels_list else ['None'], # human readable\n",
    "            'pred_labels': pred_labels_list if pred_labels_list else ['None'],\n",
    "            'true_array': true, # machine readable\n",
    "            'pred_array': pred\n",
    "        })\n",
    "\n",
    "print(f\"\\nTotal errors: {len(errors)} / {len(test_dataset)} ({len(errors)/len(test_dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fa93e",
   "metadata": {
    "id": "5c3fa93e"
   },
   "outputs": [],
   "source": [
    "# #TODO: Create error taxonomy for best performing model\n",
    "\n",
    "# Categorize errors\n",
    "error_categories = {\n",
    "    'False Negative (missed label)': 0,\n",
    "    'False Positive (extra label)': 0,\n",
    "    'Complete miss': 0,\n",
    "    'Partial (mixed FP/FN)': 0\n",
    "}\n",
    "\n",
    "for error in errors:\n",
    "    true_set = set(error['true_labels'])\n",
    "    pred_set = set(error['pred_labels'])\n",
    "    # TODO categorisation logic\n",
    "    if #...\n",
    "    elif #...\n",
    "    elif #...\n",
    "\n",
    "# Create taxonomy table\n",
    "taxonomy_df = pd.DataFrame({\n",
    "    'Error Category': list(error_categories.keys()),\n",
    "    'Count': list(error_categories.values()),\n",
    "    'Percentage': [v/len(errors)*100 for v in error_categories.values()]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + taxonomy_df.to_string(index=False))\n",
    "\n",
    "# TODO plot error distribution by category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e9530",
   "metadata": {
    "id": "731e9530"
   },
   "source": [
    "### Overall Reflection\n",
    "\n",
    "**TODO**: possible questions you could consider:\n",
    "\n",
    "1. What made this classification task difficult?\n",
    "2. Where did your model struggle most? Which classes/labels were hardest? Why?\n",
    "3. How did performance change from zero-shot → few-shot → fine-tuned?\n",
    "   Was the progression what you expected?\n",
    "4. Why did (or didn't) prompt engineering and few-shot learning help?\n",
    "5. What common mistakes did your model make? Can you identify patterns? What could you try next to improve performance? Consider:\n",
    "6. What did you learn about using small LLMs for policy analysis?\n",
    "   When are they sufficient vs. when do you need larger models or domain-specific training?\n",
    "\n",
    "[Write your reflection here] (150-300 words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
